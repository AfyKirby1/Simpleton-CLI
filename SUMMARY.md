# Simpleton CLI Project Summary

## ğŸ¯ Project Overview

**Simpleton CLI** is a local AI-powered coding agent CLI that runs entirely on your machine with local LLMs (like Ollama). Built with TypeScript and Node.js, it provides a comprehensive coding assistant experience without any cloud dependencies.

## ğŸ“Š Current Status

### âœ… Completed Features

1. **Core Architecture**
   - Modular TypeScript codebase with proper separation of concerns
   - Configuration management system with persistent storage
   - LLM client with OpenAI-compatible API support
   - Agent loop for planning and executing coding tasks

2. **File Operations**
   - File manager for reading, writing, and listing project files
   - Project context awareness with git integration
   - Beautiful diff rendering with colored output
   - Safe file operations with sandboxing

3. **Shell Operations**
   - Secure shell executor with command allow-listing
   - Protection against dangerous commands
   - Path safety checks and directory traversal prevention
   - Timeout and buffer limits for security

4. **User Interface**
   - Interactive chat mode with REPL-style interface
   - Approval prompts for user confirmation
   - Streaming responses for real-time feedback
   - Beautiful terminal UI with emojis and colors

5. **CLI Commands**
   - Main agent command with various modes (suggest, auto-edit, full-auto)
   - Interactive chat mode
   - Project initialization
   - Configuration management

6. **Security & Privacy**
   - 100% local execution - no data leaves the machine
   - Sandboxed file operations limited to project root
   - Safe command execution with allow-lists
   - Git protection warnings

## ğŸ—ï¸ Architecture Components

### Core Modules

1. **ConfigManager** (`src/config/ConfigManager.ts`)
   - Manages CLI configuration and project contexts
   - Persistent storage using `conf` library
   - Git integration for project file discovery
   - Schema validation for configuration

2. **LLMClient** (`src/llm/LLMClient.ts`)
   - OpenAI-compatible API client
   - Streaming support for real-time responses
   - Error handling and connection testing
   - Support for any local LLM endpoint

3. **AgentLoop** (`src/agent/AgentLoop.ts`)
   - Main agent logic for planning and execution
   - System prompt generation with project context
   - Response parsing and command execution
   - Integration with all other components

4. **FileManager** (`src/tools/FileManager.ts`)
   - File reading, writing, and listing operations
   - Pattern-based file discovery
   - Error handling and validation
   - Directory creation and management

5. **ShellExecutor** (`src/tools/ShellExecutor.ts`)
   - Secure command execution with allow-listing
   - Dangerous command detection
   - Path safety validation
   - Timeout and buffer management

6. **UI Components**
   - **ChatMode** (`src/ui/ChatMode.ts`) - Interactive chat interface
   - **DiffRenderer** (`src/ui/DiffRenderer.ts`) - Beautiful diff display
   - **ApprovalPrompt** (`src/ui/ApprovalPrompt.ts`) - User confirmation dialogs

7. **CLI Commands**
   - **InitCommand** (`src/commands/InitCommand.ts`) - Project initialization
   - **ConfigCommand** (`src/commands/ConfigCommand.ts`) - Configuration management

## ğŸ”§ Technical Stack

- **Language**: TypeScript 5.3+
- **Runtime**: Node.js 22+
- **Package Manager**: npm
- **Key Dependencies**:
  - `commander` - CLI argument parsing
  - `axios` - HTTP client for LLM API
  - `fs-extra` - Enhanced file system operations
  - `execa` - Secure command execution
  - `diff` - File diff generation
  - `chalk` - Terminal colors and styling
  - `conf` - Configuration management
  - `glob` - File pattern matching

## ğŸ“ Project Structure

```
ai-cli/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts              # Main CLI entry point
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ ConfigManager.ts   # Configuration management
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â””â”€â”€ AgentLoop.ts       # AI agent logic
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ LLMClient.ts       # LLM client integration
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ FileManager.ts     # File operations
â”‚   â”‚   â””â”€â”€ ShellExecutor.ts   # Shell operations
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ ChatMode.ts        # Interactive chat
â”‚   â”‚   â”œâ”€â”€ DiffRenderer.ts    # Diff display
â”‚   â”‚   â””â”€â”€ ApprovalPrompt.ts  # User prompts
â”‚   â””â”€â”€ commands/
â”‚       â”œâ”€â”€ InitCommand.ts     # Project init
â”‚       â””â”€â”€ ConfigCommand.ts   # Config management
â”œâ”€â”€ package.json               # Dependencies and scripts
â”œâ”€â”€ tsconfig.json              # TypeScript configuration
â””â”€â”€ README.md                  # Documentation
```

## ğŸš€ Usage Patterns

### Basic Usage
```bash
# Initialize project
ai-cli init

# Ask for help
ai-cli "refactor this authentication to use JWT"

# Interactive chat
ai-cli chat
```

### Advanced Modes
```bash
# Suggest only
ai-cli --suggest "add error handling"

# Auto-edit with confirmation
ai-cli --auto-edit "fix linting issues"

# Full automatic (use with caution)
ai-cli --full-auto "run tests and fix failures"
```

### Configuration
```bash
# List settings
ai-cli config --list

# Set endpoint
ai-cli config --set endpoint http://localhost:11434/v1

# Set model
ai-cli config --set model deepseek-coder:1.3b-q4_K_M
```

## ğŸ§  LLM Integration

### Supported Models
- **DeepSeek Coder** (1.3B, 6.7B variants)
- **CodeLlama** (3B, 7B, 13B variants)
- **TinyLlama** (1.1B)
- **Any OpenAI-compatible model**

### Hardware Recommendations
- **8GB RAM**: `deepseek-coder:1.3b-q4_K_M`
- **16GB+ RAM**: `deepseek-coder:6.7b-q4_K_M`
- **Fast responses**: `tinyllama:1.1b-q4_K_M`

## ğŸ”’ Security Features

1. **Local Execution**: All processing happens on localhost
2. **File Sandboxing**: Operations limited to project root
3. **Command Allow-listing**: Only safe commands allowed
4. **Path Validation**: Prevents directory traversal attacks
5. **Git Protection**: Warns about untracked directories
6. **Timeout Limits**: Prevents hanging processes

## ğŸ“ˆ Future Enhancements

### Planned Features
1. **VS Code Extension** - Integrate with VS Code
2. **Advanced Parsing** - Better response parsing for complex tasks
3. **Plugin System** - Extensible architecture for custom tools
4. **Multi-model Support** - Switch between models dynamically
5. **Project Templates** - Pre-built project configurations
6. **Performance Optimization** - Caching and optimization

### Potential Improvements
1. **Better Error Handling** - More detailed error messages
2. **Logging System** - Comprehensive logging for debugging
3. **Test Coverage** - Unit and integration tests
4. **Documentation** - API documentation and examples
5. **CI/CD Pipeline** - Automated testing and deployment

## ğŸ› Known Issues

1. **TypeScript Errors**: Some linter errors due to missing type definitions
2. **Streaming Issues**: Potential issues with streaming responses
3. **Command Parsing**: Basic command parsing that could be improved
4. **Error Recovery**: Limited error recovery mechanisms

## ğŸ¯ Success Metrics

- âœ… **100% Local Execution** - No cloud dependencies
- âœ… **Modular Architecture** - Easy to extend and maintain
- âœ… **Security First** - Safe execution environment
- âœ… **User Friendly** - Beautiful UI and clear documentation
- âœ… **Flexible Configuration** - Easy setup and customization

## ğŸ“š Documentation

- **README.md** - Comprehensive setup and usage guide
- **SUMMARY.md** - This project summary document
- **Inline Comments** - Code documentation throughout
- **Type Definitions** - TypeScript interfaces and types

## ğŸ¤ Development Guidelines

1. **Modular Design** - Keep components small and focused
2. **Type Safety** - Use TypeScript for all new code
3. **Error Handling** - Comprehensive error handling
4. **Documentation** - Document all public APIs
5. **Testing** - Add tests for new features
6. **Security** - Always consider security implications

---

**Last Updated**: December 2024
**Status**: âœ… Core functionality complete, ready for testing and refinement 